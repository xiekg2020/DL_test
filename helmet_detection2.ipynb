{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "helmet_detection2.ipynb",
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "https://github.com/xiekg2020/DL_test/blob/main/helmet_detection2.ipynb",
      "authorship_tag": "ABX9TyNIrcteL163KTPHmo6sNSpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiekg2020/DL_test/blob/efficientdet_d0_coco17/helmet_detection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U4mx45n-vB1"
      },
      "source": [
        "# **Build TF2 object detection API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrDtecd8tRH"
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNxC3xLy80mu",
        "outputId": "9eda8e9f-fc55-4a38-ef55-f8ae16ecbad8"
      },
      "source": [
        "! pip install tf_slim\n",
        "! git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 35.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 352 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 60242, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 60242 (delta 1), reused 7 (delta 0), pack-reused 60234\u001b[K\n",
            "Receiving objects: 100% (60242/60242), 573.77 MiB | 35.58 MiB/s, done.\n",
            "Resolving deltas: 100% (41927/41927), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYjpb3KC81U9"
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/slim/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/object_detection/utils/'\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/object_detection'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFKSrCsw82d1",
        "outputId": "7c6cd253-de6b-4826-8fef-444a5fda39c7"
      },
      "source": [
        "! apt-get install protobuf-compiler"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9p5_G5w88Wk",
        "outputId": "cea86576-36ef-49fa-dc1e-1a1bb471ab08"
      },
      "source": [
        "%cd models/research\n",
        "# Compile all the protobuf dependencies\n",
        "! protoc object_detection/protos/*.proto --python_out=.\n",
        "# Set up and install the object detection API\n",
        "! cp object_detection/packages/tf2/setup.py .\n",
        "! python -m pip install .\n",
        "# Run a test to make sure setup is correct\n",
        "! python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "Processing /content/models/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.31.0-cp37-cp37m-manylinux2010_x86_64.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
            "Collecting tensorflow-text>=2.5.0\n",
            "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.5.0)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 47 kB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.34.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.34.1)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.6.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.0)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.5.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, avro-python3, dill, future, seqeval\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1660509 sha256=c32fcb5cadb720a7cc9e3df124a693ae0fe3b3595d5d3f48a0f9079692fad5af\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-je6fcdrb/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=b14884690f11ad544e9a0bf89df212b16d918eafb508130e59a96a54b12f8e4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=ef52c3734884210ae0b50109e887b0e1d6975346fba146b895a02166132f8da0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=56cd999c97a3019aebacc9840a87392adf95d171e7b49bd954f4a7c0fc979790\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=174c2a7d776d4539473cec2bb182229fa60cfb7ca4669fcafa9084ffedf19eef\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=e16524bf2d625f06896796126f94f51041c8cf0228257ad5092fc14b342a9ce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo avro-python3 dill future seqeval\n",
            "Installing collected packages: requests, portalocker, future, dill, colorama, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, lvis, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.31.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.4 future-0.18.2 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.3.56 portalocker-2.3.0 py-cpuinfo-8.0.0 pyyaml-5.4.1 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.5.0 tf-models-official-2.6.0\n",
            "2021-08-17 02:33:41.002213: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Running tests under Python 3.7.11: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2021-08-17 02:33:43.538943: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-17 02:33:43.602731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:43.603348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 02:33:43.603393: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 02:33:43.799655: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-17 02:33:43.799765: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-17 02:33:43.954110: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-17 02:33:43.969978: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-17 02:33:44.217350: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-17 02:33:44.271746: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-17 02:33:44.276031: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-17 02:33:44.276220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:44.276909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:44.280427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 02:33:44.281001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:44.281581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 02:33:44.281664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:44.282319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:44.282886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 02:33:44.285871: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 02:33:49.541151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-17 02:33:49.541207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-17 02:33:49.541224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-17 02:33:49.541415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:49.542075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:49.542659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:33:49.543202: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-17 02:33:49.543255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0817 02:33:49.831528 140088099260288 model_builder.py:1088] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 6.54s\n",
            "I0817 02:33:50.070313 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 6.54s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.54s\n",
            "I0817 02:33:50.612259 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.54s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.25s\n",
            "I0817 02:33:50.865541 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.25s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.24s\n",
            "I0817 02:33:51.103457 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.24s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "W0817 02:33:51.105737 140088099260288 mobilenet_v2.py:296] `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.91s\n",
            "I0817 02:33:53.009381 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.91s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0817 02:33:53.010324 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "I0817 02:33:53.030749 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "I0817 02:33:53.044565 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "I0817 02:33:53.058961 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "I0817 02:33:53.150419 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "I0817 02:33:53.240842 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.09s\n",
            "I0817 02:33:53.333167 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "I0817 02:33:53.422549 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "I0817 02:33:53.510333 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
            "I0817 02:33:53.535187 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0817 02:33:53.698156 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0817 02:33:53.698305 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
            "I0817 02:33:53.698379 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
            "I0817 02:33:53.700558 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:53.716025 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:53.716172 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:53.766548 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:53.766691 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:53.897462 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:53.897625 140088099260288 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:33:54.028405 140088099260288 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:33:54.028573 140088099260288 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:33:54.340950 140088099260288 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:33:54.341129 140088099260288 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:33:54.531606 140088099260288 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:33:54.531771 140088099260288 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:33:54.789834 140088099260288 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:33:54.790014 140088099260288 efficientnet_model.py:147] round_filter input=320 output=320\n",
            "I0817 02:33:54.850234 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
            "I0817 02:33:54.888637 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:33:54.937006 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0817 02:33:54.937162 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 88\n",
            "I0817 02:33:54.937236 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 4\n",
            "I0817 02:33:54.938787 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:54.953364 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:54.953479 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:55.064048 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:55.064260 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:55.261096 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:55.261270 140088099260288 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:33:55.454875 140088099260288 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:33:55.455048 140088099260288 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:33:55.715122 140088099260288 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:33:55.715360 140088099260288 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:33:55.976453 140088099260288 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:33:55.976648 140088099260288 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:33:56.302680 140088099260288 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:33:56.302873 140088099260288 efficientnet_model.py:147] round_filter input=320 output=320\n",
            "I0817 02:33:56.438081 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
            "I0817 02:33:56.462435 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:33:56.520939 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0817 02:33:56.521083 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 112\n",
            "I0817 02:33:56.521152 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 5\n",
            "I0817 02:33:56.522600 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:56.534980 140088099260288 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:33:56.535089 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:56.636338 140088099260288 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:33:56.636478 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:56.947997 140088099260288 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:33:56.948181 140088099260288 efficientnet_model.py:147] round_filter input=40 output=48\n",
            "I0817 02:33:57.146175 140088099260288 efficientnet_model.py:147] round_filter input=40 output=48\n",
            "I0817 02:33:57.146339 140088099260288 efficientnet_model.py:147] round_filter input=80 output=88\n",
            "I0817 02:33:57.408326 140088099260288 efficientnet_model.py:147] round_filter input=80 output=88\n",
            "I0817 02:33:57.408494 140088099260288 efficientnet_model.py:147] round_filter input=112 output=120\n",
            "I0817 02:33:57.663237 140088099260288 efficientnet_model.py:147] round_filter input=112 output=120\n",
            "I0817 02:33:57.663415 140088099260288 efficientnet_model.py:147] round_filter input=192 output=208\n",
            "I0817 02:33:57.990450 140088099260288 efficientnet_model.py:147] round_filter input=192 output=208\n",
            "I0817 02:33:57.990612 140088099260288 efficientnet_model.py:147] round_filter input=320 output=352\n",
            "I0817 02:33:58.120158 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=1408\n",
            "I0817 02:33:58.145928 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:33:58.205173 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0817 02:33:58.205318 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 160\n",
            "I0817 02:33:58.205389 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 6\n",
            "I0817 02:33:58.206843 140088099260288 efficientnet_model.py:147] round_filter input=32 output=40\n",
            "I0817 02:33:58.221235 140088099260288 efficientnet_model.py:147] round_filter input=32 output=40\n",
            "I0817 02:33:58.221381 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:33:58.330370 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:33:58.330538 140088099260288 efficientnet_model.py:147] round_filter input=24 output=32\n",
            "I0817 02:33:58.523782 140088099260288 efficientnet_model.py:147] round_filter input=24 output=32\n",
            "I0817 02:33:58.524012 140088099260288 efficientnet_model.py:147] round_filter input=40 output=48\n",
            "I0817 02:33:58.719010 140088099260288 efficientnet_model.py:147] round_filter input=40 output=48\n",
            "I0817 02:33:58.719198 140088099260288 efficientnet_model.py:147] round_filter input=80 output=96\n",
            "I0817 02:33:59.039668 140088099260288 efficientnet_model.py:147] round_filter input=80 output=96\n",
            "I0817 02:33:59.039839 140088099260288 efficientnet_model.py:147] round_filter input=112 output=136\n",
            "I0817 02:33:59.364540 140088099260288 efficientnet_model.py:147] round_filter input=112 output=136\n",
            "I0817 02:33:59.364709 140088099260288 efficientnet_model.py:147] round_filter input=192 output=232\n",
            "I0817 02:33:59.748595 140088099260288 efficientnet_model.py:147] round_filter input=192 output=232\n",
            "I0817 02:33:59.748765 140088099260288 efficientnet_model.py:147] round_filter input=320 output=384\n",
            "I0817 02:34:00.022732 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=1536\n",
            "I0817 02:34:00.053076 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:34:00.117210 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0817 02:34:00.117366 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 224\n",
            "I0817 02:34:00.117444 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
            "I0817 02:34:00.119038 140088099260288 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "I0817 02:34:00.132723 140088099260288 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "I0817 02:34:00.132836 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:34:00.234873 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:34:00.235004 140088099260288 efficientnet_model.py:147] round_filter input=24 output=32\n",
            "I0817 02:34:00.491304 140088099260288 efficientnet_model.py:147] round_filter input=24 output=32\n",
            "I0817 02:34:00.491468 140088099260288 efficientnet_model.py:147] round_filter input=40 output=56\n",
            "I0817 02:34:00.763181 140088099260288 efficientnet_model.py:147] round_filter input=40 output=56\n",
            "I0817 02:34:00.763357 140088099260288 efficientnet_model.py:147] round_filter input=80 output=112\n",
            "I0817 02:34:01.157798 140088099260288 efficientnet_model.py:147] round_filter input=80 output=112\n",
            "I0817 02:34:01.157979 140088099260288 efficientnet_model.py:147] round_filter input=112 output=160\n",
            "I0817 02:34:01.541725 140088099260288 efficientnet_model.py:147] round_filter input=112 output=160\n",
            "I0817 02:34:01.541897 140088099260288 efficientnet_model.py:147] round_filter input=192 output=272\n",
            "I0817 02:34:02.057428 140088099260288 efficientnet_model.py:147] round_filter input=192 output=272\n",
            "I0817 02:34:02.057629 140088099260288 efficientnet_model.py:147] round_filter input=320 output=448\n",
            "I0817 02:34:02.185915 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=1792\n",
            "I0817 02:34:02.209324 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:34:02.280315 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0817 02:34:02.280456 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
            "I0817 02:34:02.280523 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
            "I0817 02:34:02.281992 140088099260288 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "I0817 02:34:02.294661 140088099260288 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "I0817 02:34:02.294770 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:34:02.458256 140088099260288 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I0817 02:34:02.458413 140088099260288 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0817 02:34:02.774321 140088099260288 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0817 02:34:02.774494 140088099260288 efficientnet_model.py:147] round_filter input=40 output=64\n",
            "I0817 02:34:03.281200 140088099260288 efficientnet_model.py:147] round_filter input=40 output=64\n",
            "I0817 02:34:03.281371 140088099260288 efficientnet_model.py:147] round_filter input=80 output=128\n",
            "I0817 02:34:03.732656 140088099260288 efficientnet_model.py:147] round_filter input=80 output=128\n",
            "I0817 02:34:03.732826 140088099260288 efficientnet_model.py:147] round_filter input=112 output=176\n",
            "I0817 02:34:04.199270 140088099260288 efficientnet_model.py:147] round_filter input=112 output=176\n",
            "I0817 02:34:04.199464 140088099260288 efficientnet_model.py:147] round_filter input=192 output=304\n",
            "I0817 02:34:04.780250 140088099260288 efficientnet_model.py:147] round_filter input=192 output=304\n",
            "I0817 02:34:04.780433 140088099260288 efficientnet_model.py:147] round_filter input=320 output=512\n",
            "I0817 02:34:04.978767 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
            "I0817 02:34:05.008199 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:34:05.095682 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0817 02:34:05.095875 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
            "I0817 02:34:05.095954 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
            "I0817 02:34:05.097640 140088099260288 efficientnet_model.py:147] round_filter input=32 output=56\n",
            "I0817 02:34:05.113228 140088099260288 efficientnet_model.py:147] round_filter input=32 output=56\n",
            "I0817 02:34:05.113351 140088099260288 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "I0817 02:34:05.264851 140088099260288 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "I0817 02:34:05.265019 140088099260288 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0817 02:34:05.650102 140088099260288 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I0817 02:34:05.650266 140088099260288 efficientnet_model.py:147] round_filter input=40 output=72\n",
            "I0817 02:34:06.032560 140088099260288 efficientnet_model.py:147] round_filter input=40 output=72\n",
            "I0817 02:34:06.032739 140088099260288 efficientnet_model.py:147] round_filter input=80 output=144\n",
            "I0817 02:34:06.556636 140088099260288 efficientnet_model.py:147] round_filter input=80 output=144\n",
            "I0817 02:34:06.556807 140088099260288 efficientnet_model.py:147] round_filter input=112 output=200\n",
            "I0817 02:34:07.239370 140088099260288 efficientnet_model.py:147] round_filter input=112 output=200\n",
            "I0817 02:34:07.239548 140088099260288 efficientnet_model.py:147] round_filter input=192 output=344\n",
            "I0817 02:34:07.939088 140088099260288 efficientnet_model.py:147] round_filter input=192 output=344\n",
            "I0817 02:34:07.939255 140088099260288 efficientnet_model.py:147] round_filter input=320 output=576\n",
            "I0817 02:34:08.129763 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=2304\n",
            "I0817 02:34:08.153692 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0817 02:34:08.251976 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0817 02:34:08.252136 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 384\n",
            "I0817 02:34:08.252210 140088099260288 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 8\n",
            "I0817 02:34:08.253673 140088099260288 efficientnet_model.py:147] round_filter input=32 output=64\n",
            "I0817 02:34:08.266803 140088099260288 efficientnet_model.py:147] round_filter input=32 output=64\n",
            "I0817 02:34:08.266945 140088099260288 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "I0817 02:34:08.481152 140088099260288 efficientnet_model.py:147] round_filter input=16 output=32\n",
            "I0817 02:34:08.481313 140088099260288 efficientnet_model.py:147] round_filter input=24 output=48\n",
            "I0817 02:34:08.929517 140088099260288 efficientnet_model.py:147] round_filter input=24 output=48\n",
            "I0817 02:34:08.929691 140088099260288 efficientnet_model.py:147] round_filter input=40 output=80\n",
            "I0817 02:34:09.377157 140088099260288 efficientnet_model.py:147] round_filter input=40 output=80\n",
            "I0817 02:34:09.377330 140088099260288 efficientnet_model.py:147] round_filter input=80 output=160\n",
            "I0817 02:34:10.011264 140088099260288 efficientnet_model.py:147] round_filter input=80 output=160\n",
            "I0817 02:34:10.011471 140088099260288 efficientnet_model.py:147] round_filter input=112 output=224\n",
            "I0817 02:34:10.863195 140088099260288 efficientnet_model.py:147] round_filter input=112 output=224\n",
            "I0817 02:34:10.863367 140088099260288 efficientnet_model.py:147] round_filter input=192 output=384\n",
            "I0817 02:34:11.699093 140088099260288 efficientnet_model.py:147] round_filter input=192 output=384\n",
            "I0817 02:34:11.699263 140088099260288 efficientnet_model.py:147] round_filter input=320 output=640\n",
            "I0817 02:34:11.953103 140088099260288 efficientnet_model.py:147] round_filter input=1280 output=2560\n",
            "I0817 02:34:11.989609 140088099260288 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 18.57s\n",
            "I0817 02:34:12.102923 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 18.57s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0817 02:34:12.110248 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0817 02:34:12.111945 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0817 02:34:12.112490 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0817 02:34:12.114058 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0817 02:34:12.115467 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0817 02:34:12.115914 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0817 02:34:12.116910 140088099260288 test_util.py:2103] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 28.591s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZ9h05M-3RS"
      },
      "source": [
        "# **Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5gSqNIH9VJa",
        "outputId": "ca7277e5-6267-4815-d59c-3c45982764bf"
      },
      "source": [
        "%mkdir /content/dataset\n",
        "%cd /content/dataset"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwivdSlH-9XL",
        "outputId": "26cdbf6c-ef5b-47c6-89e2-c7b95ca81e21"
      },
      "source": [
        "!wget -O Hard_Hat_Workers.v2-raw.tfrecord.zip https://public.roboflow.com/ds/VUqgNE4eF7?key=jWeiF3tUqx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-17 02:34:13--  https://public.roboflow.com/ds/VUqgNE4eF7?key=jWeiF3tUqx\n",
            "Resolving public.roboflow.com (public.roboflow.com)... 151.101.65.195, 151.101.1.195\n",
            "Connecting to public.roboflow.com (public.roboflow.com)|151.101.65.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com/roboflow-platform-exports/Ly2DeBzbwsemGd2ReHk4BFxy8683/TlE7G4GXJk3kU7ivmTPR/2/tfrecord.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=roboflow-platform%40appspot.gserviceaccount.com%2F20210817%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210817T023413Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=6939a36010998a19b315aded83353dfa285b5ef6abb10da1cf8c3520f50ed817586ddab64bba14de51c5b3516d4a2387e356de8293437571aba546cc4f498fb850ba6f34d36a062cfccce76ed40796a23a2f5ae045d106f7f72d3e43ebabfa0672c24d80c60a70001529affd73cbce8a3fa539da0a26e7af203e3bf6635ed624079db26c2da45586636143958178d8afb224ba53c42f74d5145b1846238ef4550c1c92e5d90d714950c201cb2dbeb050c7b374fb876ffed296e20a13e3d731a0a793fa899fdb0e186363da2730485e55533f0ae3189e08c4dc9f0405d28617f42702d629b9786acde336afb0b6c149a07131435fa15c70f71e9ef7c1495f58d2 [following]\n",
            "--2021-08-17 02:34:13--  https://storage.googleapis.com/roboflow-platform-exports/Ly2DeBzbwsemGd2ReHk4BFxy8683/TlE7G4GXJk3kU7ivmTPR/2/tfrecord.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=roboflow-platform%40appspot.gserviceaccount.com%2F20210817%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210817T023413Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=6939a36010998a19b315aded83353dfa285b5ef6abb10da1cf8c3520f50ed817586ddab64bba14de51c5b3516d4a2387e356de8293437571aba546cc4f498fb850ba6f34d36a062cfccce76ed40796a23a2f5ae045d106f7f72d3e43ebabfa0672c24d80c60a70001529affd73cbce8a3fa539da0a26e7af203e3bf6635ed624079db26c2da45586636143958178d8afb224ba53c42f74d5145b1846238ef4550c1c92e5d90d714950c201cb2dbeb050c7b374fb876ffed296e20a13e3d731a0a793fa899fdb0e186363da2730485e55533f0ae3189e08c4dc9f0405d28617f42702d629b9786acde336afb0b6c149a07131435fa15c70f71e9ef7c1495f58d2\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244284985 (233M) [application/zip]\n",
            "Saving to: ‘Hard_Hat_Workers.v2-raw.tfrecord.zip’\n",
            "\n",
            "Hard_Hat_Workers.v2 100%[===================>] 232.97M   190MB/s    in 1.2s    \n",
            "\n",
            "2021-08-17 02:34:15 (190 MB/s) - ‘Hard_Hat_Workers.v2-raw.tfrecord.zip’ saved [244284985/244284985]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4S5Lzvv_B89",
        "outputId": "f7830e67-3cfd-41fe-c7b0-cb840a104e6b"
      },
      "source": [
        "!unzip -o Hard_Hat_Workers.v2-raw.tfrecord.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Hard_Hat_Workers.v2-raw.tfrecord.zip\n",
            " extracting: test/Workers.tfrecord   \n",
            " extracting: train/Workers.tfrecord  \n",
            " extracting: test/Workers_label_map.pbtxt  \n",
            " extracting: train/Workers_label_map.pbtxt  \n",
            " extracting: README.roboflow.txt     \n",
            " extracting: README.dataset.txt      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTIqNXZy_GGM"
      },
      "source": [
        "# **Prepare model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWToxe1h_JFR",
        "outputId": "51070345-94e3-480f-fe41-0c885b204f78"
      },
      "source": [
        "!mkdir /content/pretrained_model\n",
        "%cd /content/pretrained_model\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
        "!tar xvf efficientdet_d0_coco17_tpu-32.tar.gz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pretrained_model\n",
            "--2021-08-17 02:34:17--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.142.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.142.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30736482 (29M) [application/x-tar]\n",
            "Saving to: ‘efficientdet_d0_coco17_tpu-32.tar.gz’\n",
            "\n",
            "efficientdet_d0_coc 100%[===================>]  29.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-08-17 02:34:17 (248 MB/s) - ‘efficientdet_d0_coco17_tpu-32.tar.gz’ saved [30736482/30736482]\n",
            "\n",
            "efficientdet_d0_coco17_tpu-32/\n",
            "efficientdet_d0_coco17_tpu-32/checkpoint/\n",
            "efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0.data-00000-of-00001\n",
            "efficientdet_d0_coco17_tpu-32/checkpoint/checkpoint\n",
            "efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0.index\n",
            "efficientdet_d0_coco17_tpu-32/pipeline.config\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/saved_model.pb\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/assets/\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/variables/\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/variables/variables.data-00000-of-00001\n",
            "efficientdet_d0_coco17_tpu-32/saved_model/variables/variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOv5MnZZ_v3u"
      },
      "source": [
        "PIPELINE_CONFIG_PATH = '/content/pretrained_model/efficientdet_d0_coco17_tpu-32/pipeline.config'\n",
        "PIPELINE_CHECKPOINT_PATH = '/content/pretrained_model/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30PnBSr1FJfP"
      },
      "source": [
        "# Ideally, we'd use more steps, but much larger will reach the system timeout for a free Colab environment\n",
        "# If you have Colab Pro or you're running offline, try 25000\n",
        "NUM_STEPS = 3500\n",
        "\n",
        "# Ideally, batch size would be larger, but smaller is necessary to avoid OOM Killer on free Colab environments\n",
        "# If you have Colab Pro or you're running offline, try 64\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# For this tutorial, we're training just two classes\n",
        "# If you train with the whole cats/dogs dataset, it's 37 classes\n",
        "NUM_CLASSES = 3"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEbMyFE8FPJv"
      },
      "source": [
        "import tensorflow as tf\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos import pipeline_pb2\n",
        "import os\n",
        "\n",
        "pipeline = pipeline_pb2.TrainEvalPipelineConfig()                                                                                                                                                                                                          \n",
        "config_path = PIPELINE_CONFIG_PATH\n",
        "with tf.io.gfile.GFile(config_path, \"r\") as f:                                                                                                                                                                                                                     \n",
        "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "    text_format.Merge(proto_str, pipeline)\n",
        "\n",
        "pipeline.train_input_reader.tf_record_input_reader.input_path[:] = ['/content/dataset/train/Workers.tfrecord']\n",
        "pipeline.train_input_reader.label_map_path = '/content/dataset/train/Workers_label_map.pbtxt'\n",
        "pipeline.eval_input_reader[0].tf_record_input_reader.input_path[:] = ['/content/dataset/test/Workers.tfrecord']\n",
        "pipeline.eval_input_reader[0].label_map_path = '/content/dataset/test/Workers_label_map.pbtxt'\n",
        "pipeline.train_config.fine_tune_checkpoint = PIPELINE_CHECKPOINT_PATH\n",
        "pipeline.train_config.fine_tune_checkpoint_type = 'detection'\n",
        "pipeline.train_config.batch_size = BATCH_SIZE\n",
        "pipeline.train_config.num_steps = NUM_STEPS\n",
        "pipeline.model.ssd.num_classes = NUM_CLASSES\n",
        "pipeline.model.ssd.image_resizer.fixed_shape_resizer.width = 512\n",
        "pipeline.model.ssd.image_resizer.fixed_shape_resizer.height = 512\n",
        "\n",
        "config_text = text_format.MessageToBytes(pipeline)                                                                                                                                                                                                        \n",
        "with open(config_path, \"wb\") as f:                                                                                                                                                                                                                       \n",
        "    f.write(config_text)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJDNowlBGlG4",
        "outputId": "a76996ab-5d15-447a-8be1-434cfcfb91dc"
      },
      "source": [
        "! cat $PIPELINE_CONFIG_PATH"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model {\n",
            "  ssd {\n",
            "    num_classes: 3\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 512\n",
            "        width: 512\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: \"ssd_efficientnet-b0_bifpn_keras\"\n",
            "      conv_hyperparams {\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 4e-05\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            mean: 0.0\n",
            "            stddev: 0.03\n",
            "          }\n",
            "        }\n",
            "        activation: SWISH\n",
            "        batch_norm {\n",
            "          decay: 0.99\n",
            "          scale: true\n",
            "          epsilon: 0.001\n",
            "        }\n",
            "        force_use_bias: true\n",
            "      }\n",
            "      bifpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        num_iterations: 3\n",
            "        num_filters: 64\n",
            "      }\n",
            "    }\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 1.0\n",
            "        x_scale: 1.0\n",
            "        height_scale: 1.0\n",
            "        width_scale: 1.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        conv_hyperparams {\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 4e-05\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              mean: 0.0\n",
            "              stddev: 0.01\n",
            "            }\n",
            "          }\n",
            "          activation: SWISH\n",
            "          batch_norm {\n",
            "            decay: 0.99\n",
            "            scale: true\n",
            "            epsilon: 0.001\n",
            "          }\n",
            "          force_use_bias: true\n",
            "        }\n",
            "        depth: 64\n",
            "        num_layers_before_predictor: 3\n",
            "        kernel_size: 3\n",
            "        class_prediction_bias_init: -4.6\n",
            "        use_depthwise: true\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        scales_per_octave: 3\n",
            "      }\n",
            "    }\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-08\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    loss {\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          gamma: 1.5\n",
            "          alpha: 0.25\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    add_background_class: false\n",
            "  }\n",
            "}\n",
            "train_config {\n",
            "  batch_size: 8\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_scale_crop_and_pad_to_square {\n",
            "      output_size: 512\n",
            "      scale_min: 0.1\n",
            "      scale_max: 2.0\n",
            "    }\n",
            "  }\n",
            "  sync_replicas: true\n",
            "  optimizer {\n",
            "    momentum_optimizer {\n",
            "      learning_rate {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 0.08\n",
            "          total_steps: 300000\n",
            "          warmup_learning_rate: 0.001\n",
            "          warmup_steps: 2500\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/pretrained_model/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n",
            "  num_steps: 3500\n",
            "  startup_delay_steps: 0.0\n",
            "  replicas_to_aggregate: 8\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  use_bfloat16: true\n",
            "  fine_tune_checkpoint_version: V2\n",
            "}\n",
            "train_input_reader {\n",
            "  label_map_path: \"/content/dataset/train/Workers_label_map.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/dataset/train/Workers.tfrecord\"\n",
            "  }\n",
            "}\n",
            "eval_config {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  batch_size: 1\n",
            "}\n",
            "eval_input_reader {\n",
            "  label_map_path: \"/content/dataset/test/Workers_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/dataset/test/Workers.tfrecord\"\n",
            "  }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zcLIEgBQQ2r"
      },
      "source": [
        "# **Luanch tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm1hAi2QQY9k",
        "outputId": "051df036-693f-4636-949f-6830f98e0432"
      },
      "source": [
        "%cd /content\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "--2021-08-17 02:34:18--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.197.157.142, 52.207.83.73, 34.192.220.41, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.197.157.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13832437 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  18.0MB/s    in 0.7s    \n",
            "\n",
            "2021-08-17 02:34:19 (18.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0sDOLYVQcUo",
        "outputId": "76ba0384-b372-4612-fd4c-c88d7591955a"
      },
      "source": [
        "# Starts tensorboard, so we can monitor the training process.\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format('/content/train')\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "print('Click this link to view training progress in TensorBoard:')\n",
        "import time\n",
        "time.sleep(1)\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Click this link to view training progress in TensorBoard:\n",
            "https://678239c46fda.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8JqCQiQf6l"
      },
      "source": [
        "# **Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmVXGKSuQigb"
      },
      "source": [
        "from datetime import datetime\n",
        "start = datetime.now()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiP5IubKQlDD",
        "outputId": "5d23dbad-b706-44d7-e97b-0c6652680f8e"
      },
      "source": [
        "%cd /content/models/research/\n",
        "! python3 object_detection/model_main_tf2.py \\\n",
        "    --logtostderr=true \\\n",
        "    --model_dir=/content/train \\\n",
        "    --pipeline_config_path=$PIPELINE_CONFIG_PATH"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "2021-08-17 02:34:21.970032: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 02:34:24.595201: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-17 02:34:24.623977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.624555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 02:34:24.624597: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 02:34:24.636177: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-17 02:34:24.636266: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-17 02:34:24.637793: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-17 02:34:24.638197: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-17 02:34:24.646908: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-17 02:34:24.647661: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-17 02:34:24.647925: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-17 02:34:24.648045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.648828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.649374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 02:34:24.649931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.650483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 02:34:24.650562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.651200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:24.651735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 02:34:24.651783: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 02:34:25.246840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-17 02:34:25.246900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-17 02:34:25.246916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-17 02:34:25.247103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:25.247724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:25.248333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 02:34:25.248853: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-17 02:34:25.248907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "W0817 02:34:25.250618 139849692202880 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0817 02:34:25.253261 139849692202880 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0817 02:34:25.256802 139849692202880 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0817 02:34:25.256936 139849692202880 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "I0817 02:34:25.266959 139849692202880 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0817 02:34:25.267061 139849692202880 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
            "I0817 02:34:25.267132 139849692202880 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
            "I0817 02:34:25.270807 139849692202880 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.288258 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.296239 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.298002 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.298770 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.305045 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.307735 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.312389 139849692202880 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 02:34:25.312493 139849692202880 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.324151 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.324940 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.326341 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.327106 139849692202880 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0817 02:34:25.389992 139849692202880 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 02:34:25.390091 139849692202880 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:34:25.598772 139849692202880 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 02:34:25.598927 139849692202880 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:34:25.804059 139849692202880 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 02:34:25.804214 139849692202880 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:34:26.207151 139849692202880 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 02:34:26.207308 139849692202880 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:34:26.524320 139849692202880 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 02:34:26.524471 139849692202880 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:34:26.946297 139849692202880 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 02:34:26.946463 139849692202880 efficientnet_model.py:147] round_filter input=320 output=320\n",
            "I0817 02:34:27.043530 139849692202880 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
            "I0817 02:34:27.082269 139849692202880 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0817 02:34:27.118608 139849692202880 deprecation.py:336] From /content/models/research/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/dataset/train/Workers.tfrecord']\n",
            "I0817 02:34:27.130179 139849692202880 dataset_builder.py:163] Reading unweighted datasets: ['/content/dataset/train/Workers.tfrecord']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/dataset/train/Workers.tfrecord']\n",
            "I0817 02:34:27.130357 139849692202880 dataset_builder.py:80] Reading record datasets for input file: ['/content/dataset/train/Workers.tfrecord']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0817 02:34:27.130440 139849692202880 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0817 02:34:27.130513 139849692202880 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "W0817 02:34:27.142980 139849692202880 deprecation.py:336] From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0817 02:34:27.164920 139849692202880 deprecation.py:336] From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0817 02:34:33.816545 139849692202880 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0817 02:34:37.599660 139849692202880 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-08-17 02:34:39.745444: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-08-17 02:34:39.751854: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "2021-08-17 02:35:03.634337: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-17 02:35:05.824880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8004\n",
            "2021-08-17 02:35:30.116008: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-17 02:35:32.590358: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0817 02:35:37.027359 139846045886208 deprecation.py:534] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "W0817 02:35:44.236189 139846045886208 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "W0817 02:35:54.619883 139846045886208 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "W0817 02:36:04.412826 139846045886208 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "W0817 02:36:14.623952 139846045886208 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss.\n",
            "INFO:tensorflow:Step 100 per-step time 1.082s\n",
            "I0817 02:37:24.850497 139849692202880 model_lib_v2.py:700] Step 100 per-step time 1.082s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.5717526,\n",
            " 'Loss/localization_loss': 0.03365879,\n",
            " 'Loss/regularization_loss': 0.02834481,\n",
            " 'Loss/total_loss': 0.6337562,\n",
            " 'learning_rate': 0.00416}\n",
            "I0817 02:37:24.850827 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.5717526,\n",
            " 'Loss/localization_loss': 0.03365879,\n",
            " 'Loss/regularization_loss': 0.02834481,\n",
            " 'Loss/total_loss': 0.6337562,\n",
            " 'learning_rate': 0.00416}\n",
            "INFO:tensorflow:Step 200 per-step time 0.525s\n",
            "I0817 02:38:17.356906 139849692202880 model_lib_v2.py:700] Step 200 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.4330049,\n",
            " 'Loss/localization_loss': 0.013396737,\n",
            " 'Loss/regularization_loss': 0.028351631,\n",
            " 'Loss/total_loss': 0.47475326,\n",
            " 'learning_rate': 0.0073200003}\n",
            "I0817 02:38:17.357207 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.4330049,\n",
            " 'Loss/localization_loss': 0.013396737,\n",
            " 'Loss/regularization_loss': 0.028351631,\n",
            " 'Loss/total_loss': 0.47475326,\n",
            " 'learning_rate': 0.0073200003}\n",
            "INFO:tensorflow:Step 300 per-step time 0.527s\n",
            "I0817 02:39:10.088665 139849692202880 model_lib_v2.py:700] Step 300 per-step time 0.527s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.4037382,\n",
            " 'Loss/localization_loss': 0.012968403,\n",
            " 'Loss/regularization_loss': 0.028362835,\n",
            " 'Loss/total_loss': 0.44506943,\n",
            " 'learning_rate': 0.010480001}\n",
            "I0817 02:39:10.088962 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.4037382,\n",
            " 'Loss/localization_loss': 0.012968403,\n",
            " 'Loss/regularization_loss': 0.028362835,\n",
            " 'Loss/total_loss': 0.44506943,\n",
            " 'learning_rate': 0.010480001}\n",
            "INFO:tensorflow:Step 400 per-step time 0.528s\n",
            "I0817 02:40:02.907850 139849692202880 model_lib_v2.py:700] Step 400 per-step time 0.528s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.52817065,\n",
            " 'Loss/localization_loss': 0.013190249,\n",
            " 'Loss/regularization_loss': 0.028381959,\n",
            " 'Loss/total_loss': 0.56974286,\n",
            " 'learning_rate': 0.0136400005}\n",
            "I0817 02:40:02.908176 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.52817065,\n",
            " 'Loss/localization_loss': 0.013190249,\n",
            " 'Loss/regularization_loss': 0.028381959,\n",
            " 'Loss/total_loss': 0.56974286,\n",
            " 'learning_rate': 0.0136400005}\n",
            "INFO:tensorflow:Step 500 per-step time 0.529s\n",
            "I0817 02:40:55.773876 139849692202880 model_lib_v2.py:700] Step 500 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2438219,\n",
            " 'Loss/localization_loss': 0.0064413836,\n",
            " 'Loss/regularization_loss': 0.028404888,\n",
            " 'Loss/total_loss': 0.27866817,\n",
            " 'learning_rate': 0.016800001}\n",
            "I0817 02:40:55.774174 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2438219,\n",
            " 'Loss/localization_loss': 0.0064413836,\n",
            " 'Loss/regularization_loss': 0.028404888,\n",
            " 'Loss/total_loss': 0.27866817,\n",
            " 'learning_rate': 0.016800001}\n",
            "INFO:tensorflow:Step 600 per-step time 0.530s\n",
            "I0817 02:41:48.723748 139849692202880 model_lib_v2.py:700] Step 600 per-step time 0.530s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.30842215,\n",
            " 'Loss/localization_loss': 0.007857906,\n",
            " 'Loss/regularization_loss': 0.028447604,\n",
            " 'Loss/total_loss': 0.34472767,\n",
            " 'learning_rate': 0.019960001}\n",
            "I0817 02:41:48.724055 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.30842215,\n",
            " 'Loss/localization_loss': 0.007857906,\n",
            " 'Loss/regularization_loss': 0.028447604,\n",
            " 'Loss/total_loss': 0.34472767,\n",
            " 'learning_rate': 0.019960001}\n",
            "INFO:tensorflow:Step 700 per-step time 0.529s\n",
            "I0817 02:42:41.603656 139849692202880 model_lib_v2.py:700] Step 700 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.47435638,\n",
            " 'Loss/localization_loss': 0.010320189,\n",
            " 'Loss/regularization_loss': 0.028482566,\n",
            " 'Loss/total_loss': 0.51315916,\n",
            " 'learning_rate': 0.023120001}\n",
            "I0817 02:42:41.603974 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.47435638,\n",
            " 'Loss/localization_loss': 0.010320189,\n",
            " 'Loss/regularization_loss': 0.028482566,\n",
            " 'Loss/total_loss': 0.51315916,\n",
            " 'learning_rate': 0.023120001}\n",
            "INFO:tensorflow:Step 800 per-step time 0.528s\n",
            "I0817 02:43:34.389459 139849692202880 model_lib_v2.py:700] Step 800 per-step time 0.528s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.37323254,\n",
            " 'Loss/localization_loss': 0.008668526,\n",
            " 'Loss/regularization_loss': 0.028560704,\n",
            " 'Loss/total_loss': 0.41046175,\n",
            " 'learning_rate': 0.02628}\n",
            "I0817 02:43:34.389801 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.37323254,\n",
            " 'Loss/localization_loss': 0.008668526,\n",
            " 'Loss/regularization_loss': 0.028560704,\n",
            " 'Loss/total_loss': 0.41046175,\n",
            " 'learning_rate': 0.02628}\n",
            "INFO:tensorflow:Step 900 per-step time 0.530s\n",
            "I0817 02:44:27.355646 139849692202880 model_lib_v2.py:700] Step 900 per-step time 0.530s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33700657,\n",
            " 'Loss/localization_loss': 0.007675487,\n",
            " 'Loss/regularization_loss': 0.028632794,\n",
            " 'Loss/total_loss': 0.37331486,\n",
            " 'learning_rate': 0.02944}\n",
            "I0817 02:44:27.355937 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.33700657,\n",
            " 'Loss/localization_loss': 0.007675487,\n",
            " 'Loss/regularization_loss': 0.028632794,\n",
            " 'Loss/total_loss': 0.37331486,\n",
            " 'learning_rate': 0.02944}\n",
            "INFO:tensorflow:Step 1000 per-step time 0.529s\n",
            "I0817 02:45:20.238382 139849692202880 model_lib_v2.py:700] Step 1000 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2998269,\n",
            " 'Loss/localization_loss': 0.0055681774,\n",
            " 'Loss/regularization_loss': 0.02869753,\n",
            " 'Loss/total_loss': 0.3340926,\n",
            " 'learning_rate': 0.0326}\n",
            "I0817 02:45:20.238664 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2998269,\n",
            " 'Loss/localization_loss': 0.0055681774,\n",
            " 'Loss/regularization_loss': 0.02869753,\n",
            " 'Loss/total_loss': 0.3340926,\n",
            " 'learning_rate': 0.0326}\n",
            "INFO:tensorflow:Step 1100 per-step time 0.535s\n",
            "I0817 02:46:13.757844 139849692202880 model_lib_v2.py:700] Step 1100 per-step time 0.535s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.29793933,\n",
            " 'Loss/localization_loss': 0.0059626955,\n",
            " 'Loss/regularization_loss': 0.028766088,\n",
            " 'Loss/total_loss': 0.33266813,\n",
            " 'learning_rate': 0.03576}\n",
            "I0817 02:46:13.758161 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.29793933,\n",
            " 'Loss/localization_loss': 0.0059626955,\n",
            " 'Loss/regularization_loss': 0.028766088,\n",
            " 'Loss/total_loss': 0.33266813,\n",
            " 'learning_rate': 0.03576}\n",
            "INFO:tensorflow:Step 1200 per-step time 0.525s\n",
            "I0817 02:47:06.292074 139849692202880 model_lib_v2.py:700] Step 1200 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2558288,\n",
            " 'Loss/localization_loss': 0.0071871546,\n",
            " 'Loss/regularization_loss': 0.028834311,\n",
            " 'Loss/total_loss': 0.29185027,\n",
            " 'learning_rate': 0.03892}\n",
            "I0817 02:47:06.292383 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2558288,\n",
            " 'Loss/localization_loss': 0.0071871546,\n",
            " 'Loss/regularization_loss': 0.028834311,\n",
            " 'Loss/total_loss': 0.29185027,\n",
            " 'learning_rate': 0.03892}\n",
            "INFO:tensorflow:Step 1300 per-step time 0.529s\n",
            "I0817 02:47:59.221307 139849692202880 model_lib_v2.py:700] Step 1300 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.25588596,\n",
            " 'Loss/localization_loss': 0.0053007575,\n",
            " 'Loss/regularization_loss': 0.028912019,\n",
            " 'Loss/total_loss': 0.29009873,\n",
            " 'learning_rate': 0.04208}\n",
            "I0817 02:47:59.221611 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.25588596,\n",
            " 'Loss/localization_loss': 0.0053007575,\n",
            " 'Loss/regularization_loss': 0.028912019,\n",
            " 'Loss/total_loss': 0.29009873,\n",
            " 'learning_rate': 0.04208}\n",
            "INFO:tensorflow:Step 1400 per-step time 0.528s\n",
            "I0817 02:48:52.016399 139849692202880 model_lib_v2.py:700] Step 1400 per-step time 0.528s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.24433033,\n",
            " 'Loss/localization_loss': 0.007843326,\n",
            " 'Loss/regularization_loss': 0.029002776,\n",
            " 'Loss/total_loss': 0.28117645,\n",
            " 'learning_rate': 0.04524}\n",
            "I0817 02:48:52.016705 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.24433033,\n",
            " 'Loss/localization_loss': 0.007843326,\n",
            " 'Loss/regularization_loss': 0.029002776,\n",
            " 'Loss/total_loss': 0.28117645,\n",
            " 'learning_rate': 0.04524}\n",
            "INFO:tensorflow:Step 1500 per-step time 0.527s\n",
            "I0817 02:49:44.724559 139849692202880 model_lib_v2.py:700] Step 1500 per-step time 0.527s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26949528,\n",
            " 'Loss/localization_loss': 0.0074700997,\n",
            " 'Loss/regularization_loss': 0.029113218,\n",
            " 'Loss/total_loss': 0.3060786,\n",
            " 'learning_rate': 0.0484}\n",
            "I0817 02:49:44.724849 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.26949528,\n",
            " 'Loss/localization_loss': 0.0074700997,\n",
            " 'Loss/regularization_loss': 0.029113218,\n",
            " 'Loss/total_loss': 0.3060786,\n",
            " 'learning_rate': 0.0484}\n",
            "INFO:tensorflow:Step 1600 per-step time 0.526s\n",
            "I0817 02:50:37.345993 139849692202880 model_lib_v2.py:700] Step 1600 per-step time 0.526s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3263267,\n",
            " 'Loss/localization_loss': 0.007303565,\n",
            " 'Loss/regularization_loss': 0.02926106,\n",
            " 'Loss/total_loss': 0.36289132,\n",
            " 'learning_rate': 0.05156}\n",
            "I0817 02:50:37.346297 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.3263267,\n",
            " 'Loss/localization_loss': 0.007303565,\n",
            " 'Loss/regularization_loss': 0.02926106,\n",
            " 'Loss/total_loss': 0.36289132,\n",
            " 'learning_rate': 0.05156}\n",
            "INFO:tensorflow:Step 1700 per-step time 0.529s\n",
            "I0817 02:51:30.265608 139849692202880 model_lib_v2.py:700] Step 1700 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26385182,\n",
            " 'Loss/localization_loss': 0.004482115,\n",
            " 'Loss/regularization_loss': 0.029469272,\n",
            " 'Loss/total_loss': 0.29780322,\n",
            " 'learning_rate': 0.05472}\n",
            "I0817 02:51:30.265915 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.26385182,\n",
            " 'Loss/localization_loss': 0.004482115,\n",
            " 'Loss/regularization_loss': 0.029469272,\n",
            " 'Loss/total_loss': 0.29780322,\n",
            " 'learning_rate': 0.05472}\n",
            "INFO:tensorflow:Step 1800 per-step time 0.531s\n",
            "I0817 02:52:23.325581 139849692202880 model_lib_v2.py:700] Step 1800 per-step time 0.531s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3690811,\n",
            " 'Loss/localization_loss': 0.0058493856,\n",
            " 'Loss/regularization_loss': 0.029668637,\n",
            " 'Loss/total_loss': 0.40459913,\n",
            " 'learning_rate': 0.05788}\n",
            "I0817 02:52:23.325868 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.3690811,\n",
            " 'Loss/localization_loss': 0.0058493856,\n",
            " 'Loss/regularization_loss': 0.029668637,\n",
            " 'Loss/total_loss': 0.40459913,\n",
            " 'learning_rate': 0.05788}\n",
            "INFO:tensorflow:Step 1900 per-step time 0.529s\n",
            "I0817 02:53:16.234853 139849692202880 model_lib_v2.py:700] Step 1900 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23423533,\n",
            " 'Loss/localization_loss': 0.005713764,\n",
            " 'Loss/regularization_loss': 0.029814709,\n",
            " 'Loss/total_loss': 0.2697638,\n",
            " 'learning_rate': 0.06104}\n",
            "I0817 02:53:16.235167 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.23423533,\n",
            " 'Loss/localization_loss': 0.005713764,\n",
            " 'Loss/regularization_loss': 0.029814709,\n",
            " 'Loss/total_loss': 0.2697638,\n",
            " 'learning_rate': 0.06104}\n",
            "INFO:tensorflow:Step 2000 per-step time 0.527s\n",
            "I0817 02:54:08.971061 139849692202880 model_lib_v2.py:700] Step 2000 per-step time 0.527s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2122729,\n",
            " 'Loss/localization_loss': 0.0075786444,\n",
            " 'Loss/regularization_loss': 0.02992261,\n",
            " 'Loss/total_loss': 0.24977414,\n",
            " 'learning_rate': 0.06420001}\n",
            "I0817 02:54:08.971353 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2122729,\n",
            " 'Loss/localization_loss': 0.0075786444,\n",
            " 'Loss/regularization_loss': 0.02992261,\n",
            " 'Loss/total_loss': 0.24977414,\n",
            " 'learning_rate': 0.06420001}\n",
            "INFO:tensorflow:Step 2100 per-step time 0.539s\n",
            "I0817 02:55:02.882197 139849692202880 model_lib_v2.py:700] Step 2100 per-step time 0.539s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.27841526,\n",
            " 'Loss/localization_loss': 0.0050048237,\n",
            " 'Loss/regularization_loss': 0.030070364,\n",
            " 'Loss/total_loss': 0.31349045,\n",
            " 'learning_rate': 0.067360006}\n",
            "I0817 02:55:02.882508 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.27841526,\n",
            " 'Loss/localization_loss': 0.0050048237,\n",
            " 'Loss/regularization_loss': 0.030070364,\n",
            " 'Loss/total_loss': 0.31349045,\n",
            " 'learning_rate': 0.067360006}\n",
            "INFO:tensorflow:Step 2200 per-step time 0.530s\n",
            "I0817 02:55:55.920827 139849692202880 model_lib_v2.py:700] Step 2200 per-step time 0.530s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23485482,\n",
            " 'Loss/localization_loss': 0.004714946,\n",
            " 'Loss/regularization_loss': 0.030232888,\n",
            " 'Loss/total_loss': 0.26980266,\n",
            " 'learning_rate': 0.070520006}\n",
            "I0817 02:55:55.921125 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.23485482,\n",
            " 'Loss/localization_loss': 0.004714946,\n",
            " 'Loss/regularization_loss': 0.030232888,\n",
            " 'Loss/total_loss': 0.26980266,\n",
            " 'learning_rate': 0.070520006}\n",
            "INFO:tensorflow:Step 2300 per-step time 0.533s\n",
            "I0817 02:56:49.256967 139849692202880 model_lib_v2.py:700] Step 2300 per-step time 0.533s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.31565902,\n",
            " 'Loss/localization_loss': 0.0072483066,\n",
            " 'Loss/regularization_loss': 0.030412255,\n",
            " 'Loss/total_loss': 0.3533196,\n",
            " 'learning_rate': 0.073680006}\n",
            "I0817 02:56:49.257263 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.31565902,\n",
            " 'Loss/localization_loss': 0.0072483066,\n",
            " 'Loss/regularization_loss': 0.030412255,\n",
            " 'Loss/total_loss': 0.3533196,\n",
            " 'learning_rate': 0.073680006}\n",
            "INFO:tensorflow:Step 2400 per-step time 0.529s\n",
            "I0817 02:57:42.152335 139849692202880 model_lib_v2.py:700] Step 2400 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2555271,\n",
            " 'Loss/localization_loss': 0.0048875413,\n",
            " 'Loss/regularization_loss': 0.030632712,\n",
            " 'Loss/total_loss': 0.29104736,\n",
            " 'learning_rate': 0.076840006}\n",
            "I0817 02:57:42.152628 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2555271,\n",
            " 'Loss/localization_loss': 0.0048875413,\n",
            " 'Loss/regularization_loss': 0.030632712,\n",
            " 'Loss/total_loss': 0.29104736,\n",
            " 'learning_rate': 0.076840006}\n",
            "INFO:tensorflow:Step 2500 per-step time 0.528s\n",
            "I0817 02:58:34.910337 139849692202880 model_lib_v2.py:700] Step 2500 per-step time 0.528s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.22712673,\n",
            " 'Loss/localization_loss': 0.006060669,\n",
            " 'Loss/regularization_loss': 0.030754395,\n",
            " 'Loss/total_loss': 0.2639418,\n",
            " 'learning_rate': 0.08}\n",
            "I0817 02:58:34.910630 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.22712673,\n",
            " 'Loss/localization_loss': 0.006060669,\n",
            " 'Loss/regularization_loss': 0.030754395,\n",
            " 'Loss/total_loss': 0.2639418,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 2600 per-step time 0.525s\n",
            "I0817 02:59:27.452002 139849692202880 model_lib_v2.py:700] Step 2600 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.35824463,\n",
            " 'Loss/localization_loss': 0.005951761,\n",
            " 'Loss/regularization_loss': 0.030953342,\n",
            " 'Loss/total_loss': 0.39514974,\n",
            " 'learning_rate': 0.079999976}\n",
            "I0817 02:59:27.452303 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.35824463,\n",
            " 'Loss/localization_loss': 0.005951761,\n",
            " 'Loss/regularization_loss': 0.030953342,\n",
            " 'Loss/total_loss': 0.39514974,\n",
            " 'learning_rate': 0.079999976}\n",
            "INFO:tensorflow:Step 2700 per-step time 0.527s\n",
            "I0817 03:00:20.184753 139849692202880 model_lib_v2.py:700] Step 2700 per-step time 0.527s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.19862513,\n",
            " 'Loss/localization_loss': 0.0031821672,\n",
            " 'Loss/regularization_loss': 0.031062853,\n",
            " 'Loss/total_loss': 0.23287016,\n",
            " 'learning_rate': 0.07999991}\n",
            "I0817 03:00:20.185067 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.19862513,\n",
            " 'Loss/localization_loss': 0.0031821672,\n",
            " 'Loss/regularization_loss': 0.031062853,\n",
            " 'Loss/total_loss': 0.23287016,\n",
            " 'learning_rate': 0.07999991}\n",
            "INFO:tensorflow:Step 2800 per-step time 0.528s\n",
            "I0817 03:01:12.971094 139849692202880 model_lib_v2.py:700] Step 2800 per-step time 0.528s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.22612956,\n",
            " 'Loss/localization_loss': 0.0057497113,\n",
            " 'Loss/regularization_loss': 0.031221643,\n",
            " 'Loss/total_loss': 0.26310092,\n",
            " 'learning_rate': 0.0799998}\n",
            "I0817 03:01:12.971392 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.22612956,\n",
            " 'Loss/localization_loss': 0.0057497113,\n",
            " 'Loss/regularization_loss': 0.031221643,\n",
            " 'Loss/total_loss': 0.26310092,\n",
            " 'learning_rate': 0.0799998}\n",
            "INFO:tensorflow:Step 2900 per-step time 0.527s\n",
            "I0817 03:02:05.692911 139849692202880 model_lib_v2.py:700] Step 2900 per-step time 0.527s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.31614757,\n",
            " 'Loss/localization_loss': 0.006622917,\n",
            " 'Loss/regularization_loss': 0.03136188,\n",
            " 'Loss/total_loss': 0.35413235,\n",
            " 'learning_rate': 0.07999964}\n",
            "I0817 03:02:05.693203 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.31614757,\n",
            " 'Loss/localization_loss': 0.006622917,\n",
            " 'Loss/regularization_loss': 0.03136188,\n",
            " 'Loss/total_loss': 0.35413235,\n",
            " 'learning_rate': 0.07999964}\n",
            "INFO:tensorflow:Step 3000 per-step time 0.529s\n",
            "I0817 03:02:58.640722 139849692202880 model_lib_v2.py:700] Step 3000 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2129563,\n",
            " 'Loss/localization_loss': 0.00486663,\n",
            " 'Loss/regularization_loss': 0.03149815,\n",
            " 'Loss/total_loss': 0.24932107,\n",
            " 'learning_rate': 0.07999944}\n",
            "I0817 03:02:58.641031 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.2129563,\n",
            " 'Loss/localization_loss': 0.00486663,\n",
            " 'Loss/regularization_loss': 0.03149815,\n",
            " 'Loss/total_loss': 0.24932107,\n",
            " 'learning_rate': 0.07999944}\n",
            "INFO:tensorflow:Step 3100 per-step time 0.538s\n",
            "I0817 03:03:52.400563 139849692202880 model_lib_v2.py:700] Step 3100 per-step time 0.538s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1947532,\n",
            " 'Loss/localization_loss': 0.00322385,\n",
            " 'Loss/regularization_loss': 0.031626187,\n",
            " 'Loss/total_loss': 0.22960323,\n",
            " 'learning_rate': 0.07999919}\n",
            "I0817 03:03:52.400875 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.1947532,\n",
            " 'Loss/localization_loss': 0.00322385,\n",
            " 'Loss/regularization_loss': 0.031626187,\n",
            " 'Loss/total_loss': 0.22960323,\n",
            " 'learning_rate': 0.07999919}\n",
            "INFO:tensorflow:Step 3200 per-step time 0.524s\n",
            "I0817 03:04:44.755352 139849692202880 model_lib_v2.py:700] Step 3200 per-step time 0.524s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.28088126,\n",
            " 'Loss/localization_loss': 0.0048516733,\n",
            " 'Loss/regularization_loss': 0.031732146,\n",
            " 'Loss/total_loss': 0.31746507,\n",
            " 'learning_rate': 0.0799989}\n",
            "I0817 03:04:44.755660 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.28088126,\n",
            " 'Loss/localization_loss': 0.0048516733,\n",
            " 'Loss/regularization_loss': 0.031732146,\n",
            " 'Loss/total_loss': 0.31746507,\n",
            " 'learning_rate': 0.0799989}\n",
            "INFO:tensorflow:Step 3300 per-step time 0.530s\n",
            "I0817 03:05:37.709752 139849692202880 model_lib_v2.py:700] Step 3300 per-step time 0.530s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16728134,\n",
            " 'Loss/localization_loss': 0.0027281232,\n",
            " 'Loss/regularization_loss': 0.031842303,\n",
            " 'Loss/total_loss': 0.20185177,\n",
            " 'learning_rate': 0.07999857}\n",
            "I0817 03:05:37.710102 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.16728134,\n",
            " 'Loss/localization_loss': 0.0027281232,\n",
            " 'Loss/regularization_loss': 0.031842303,\n",
            " 'Loss/total_loss': 0.20185177,\n",
            " 'learning_rate': 0.07999857}\n",
            "INFO:tensorflow:Step 3400 per-step time 0.529s\n",
            "I0817 03:06:30.626109 139849692202880 model_lib_v2.py:700] Step 3400 per-step time 0.529s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.27962762,\n",
            " 'Loss/localization_loss': 0.0051979995,\n",
            " 'Loss/regularization_loss': 0.03193063,\n",
            " 'Loss/total_loss': 0.31675625,\n",
            " 'learning_rate': 0.07999819}\n",
            "I0817 03:06:30.626399 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.27962762,\n",
            " 'Loss/localization_loss': 0.0051979995,\n",
            " 'Loss/regularization_loss': 0.03193063,\n",
            " 'Loss/total_loss': 0.31675625,\n",
            " 'learning_rate': 0.07999819}\n",
            "INFO:tensorflow:Step 3500 per-step time 0.525s\n",
            "I0817 03:07:23.168769 139849692202880 model_lib_v2.py:700] Step 3500 per-step time 0.525s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.32351345,\n",
            " 'Loss/localization_loss': 0.0067787752,\n",
            " 'Loss/regularization_loss': 0.032010496,\n",
            " 'Loss/total_loss': 0.36230272,\n",
            " 'learning_rate': 0.07999776}\n",
            "I0817 03:07:23.169076 139849692202880 model_lib_v2.py:701] {'Loss/classification_loss': 0.32351345,\n",
            " 'Loss/localization_loss': 0.0067787752,\n",
            " 'Loss/regularization_loss': 0.032010496,\n",
            " 'Loss/total_loss': 0.36230272,\n",
            " 'learning_rate': 0.07999776}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hic8L3nQpcT",
        "outputId": "28dc84ca-37ee-4b44-a0e3-2a922b666051"
      },
      "source": [
        "end = datetime.now()\n",
        "duration = end - start\n",
        "seconds_in_hour = 60 * 60\n",
        "hours, seconds = divmod(duration.seconds, seconds_in_hour)\n",
        "minutes = int(seconds / 60)\n",
        "print('TRAINING TIME:', str(hours) + ':' + str(minutes if minutes > 10 else '%02d' % minutes))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING TIME: 0:33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rsKWVLas1bA"
      },
      "source": [
        "# **Export the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGz67cjms_7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbad479-c431-4d1a-be5b-abcd6246ba7b"
      },
      "source": [
        "! python3 /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "    --output_directory=/content/inference_graph \\\n",
        "    --trained_checkpoint_dir=/content/train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-17 03:07:26.927577: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 03:07:28.796284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-17 03:07:28.828850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.829446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 03:07:28.829482: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 03:07:28.831665: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-17 03:07:28.831739: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-17 03:07:28.833343: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-17 03:07:28.833659: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-17 03:07:28.835588: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-17 03:07:28.836464: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-17 03:07:28.836645: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-17 03:07:28.836737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.837352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.837907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 03:07:28.838390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.838949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-08-17 03:07:28.839026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.839595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:28.840143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-17 03:07:28.840196: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-17 03:07:29.429903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-17 03:07:29.429955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-17 03:07:29.429972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-17 03:07:29.430153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:29.430765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:29.431367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:07:29.431890: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-17 03:07:29.431936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0817 03:07:29.438425 140367213307776 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0817 03:07:29.438612 140367213307776 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
            "I0817 03:07:29.438685 140367213307776 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
            "I0817 03:07:29.441704 140367213307776 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 03:07:29.459193 140367213307776 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 03:07:29.459298 140367213307776 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 03:07:29.508717 140367213307776 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 03:07:29.508834 140367213307776 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 03:07:29.629365 140367213307776 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 03:07:29.629485 140367213307776 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 03:07:29.754054 140367213307776 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 03:07:29.754179 140367213307776 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 03:07:29.949821 140367213307776 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 03:07:29.950003 140367213307776 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 03:07:30.149689 140367213307776 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 03:07:30.149856 140367213307776 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 03:07:30.405729 140367213307776 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 03:07:30.405913 140367213307776 efficientnet_model.py:147] round_filter input=320 output=320\n",
            "I0817 03:07:30.464488 140367213307776 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
            "I0817 03:07:30.490067 140367213307776 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "W0817 03:07:32.515091 140367213307776 deprecation.py:601] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa95007dc50>, because it is not built.\n",
            "W0817 03:07:48.772752 140367213307776 save_impl.py:77] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7fa95007dc50>, because it is not built.\n",
            "2021-08-17 03:08:09.709666: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0817 03:08:43.197492 140367213307776 save.py:243] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 795). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "W0817 03:08:51.247696 140367213307776 save.py:1240] FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
            "INFO:tensorflow:Assets written to: /content/inference_graph/saved_model/assets\n",
            "I0817 03:08:52.015127 140367213307776 builder_impl.py:775] Assets written to: /content/inference_graph/saved_model/assets\n",
            "INFO:tensorflow:Writing pipeline config file to /content/inference_graph/pipeline.config\n",
            "I0817 03:08:53.470187 140367213307776 config_util.py:254] Writing pipeline config file to /content/inference_graph/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3oKCHxvBdpt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6acc9ad-edcd-4950-a6e1-a9b3786e3ab7"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Collecting tensorboard~=2.6\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 57.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Collecting keras~=2.6\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 75.2 MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "  Downloading grpcio-1.39.0-cp37-cp37m-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.34.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n",
            "Building wheels for collected packages: clang\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=6ca4f6c238faa784188e1b91d0a66440965096019ea9eb9185d533dad03e98b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "Successfully built clang\n",
            "Installing collected packages: grpcio, tensorflow-estimator, tensorboard, keras, clang, tensorflow\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.5.0 requires tensorflow<2.6,>=2.5.0, but you have tensorflow 2.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed clang-5.0 grpcio-1.39.0 keras-2.6.0 tensorboard-2.6.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MKsn7DjVvzM"
      },
      "source": [
        "OUTPUT_DIR = '/content/output_efficientdet_d0_coco17_tpu-32_hard_hat'\n",
        "! mkdir $OUTPUT_DIR"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7M5kaaqWCAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1e15f4-a152-4c8d-aadd-8630733b0d29"
      },
      "source": [
        "!python3 object_detection/export_tflite_graph_tf2.py  \\\n",
        "--pipeline_config_path=/content/inference_graph/pipeline.config \\\n",
        "--trained_checkpoint_dir=/content/train \\\n",
        "--output_directory=/content/output_efficientdet_d0_coco17_tpu-32_hard_hat"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-17 03:10:19.877819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:10:19.910164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2021-08-17 03:10:19.916001: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-08-17 03:10:19.933500: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "I0817 03:10:19.940212 140152776742784 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0817 03:10:19.940446 140152776742784 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 64\n",
            "I0817 03:10:19.940523 140152776742784 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 3\n",
            "I0817 03:10:19.944331 140152776742784 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 03:10:19.964155 140152776742784 efficientnet_model.py:147] round_filter input=32 output=32\n",
            "I0817 03:10:19.964271 140152776742784 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 03:10:20.014740 140152776742784 efficientnet_model.py:147] round_filter input=16 output=16\n",
            "I0817 03:10:20.014848 140152776742784 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 03:10:20.145212 140152776742784 efficientnet_model.py:147] round_filter input=24 output=24\n",
            "I0817 03:10:20.145358 140152776742784 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 03:10:20.282727 140152776742784 efficientnet_model.py:147] round_filter input=40 output=40\n",
            "I0817 03:10:20.282933 140152776742784 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 03:10:20.498219 140152776742784 efficientnet_model.py:147] round_filter input=80 output=80\n",
            "I0817 03:10:20.498386 140152776742784 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 03:10:20.710646 140152776742784 efficientnet_model.py:147] round_filter input=112 output=112\n",
            "I0817 03:10:20.710804 140152776742784 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 03:10:21.115577 140152776742784 efficientnet_model.py:147] round_filter input=192 output=192\n",
            "I0817 03:10:21.115748 140152776742784 efficientnet_model.py:147] round_filter input=320 output=320\n",
            "I0817 03:10:21.195151 140152776742784 efficientnet_model.py:147] round_filter input=1280 output=1280\n",
            "I0817 03:10:21.245122 140152776742784 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f7736d344d0>, because it is not built.\n",
            "W0817 03:10:33.193957 140152776742784 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f7736d344d0>, because it is not built.\n",
            "2021-08-17 03:10:56.196771: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0817 03:11:30.864955 140152776742784 save.py:254] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 795). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: /content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/assets\n",
            "I0817 03:11:39.276573 140152776742784 builder_impl.py:781] Assets written to: /content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c0BihDjZhap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb05029-31d8-417b-ed79-823b6a475837"
      },
      "source": [
        "!tflite_convert \\\n",
        "--saved_model_dir=/content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model \\\n",
        "--output_file=/content/output_efficientdet_d0_coco17_tpu-32_hard_hat/detect.tflite \\\n",
        "--input_shapes=1,640,640,3 \\\n",
        "--input_arrays=normalized_input_image_tensor \\\n",
        "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
        "--inference_type=FLOAT \\\n",
        "--allow_custom_ops"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-17 03:11:45.468130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-17 03:11:45.483880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2021-08-17 03:11:45.484643: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-08-17 03:11:46.630763: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "W0817 03:11:46.820324 140059710097280 function_deserialization.py:572] Importing a function (__inference_EfficientDet-D0_layer_call_and_return_conditional_losses_91750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:48.068753 140059710097280 function_deserialization.py:572] Importing a function (__inference_EfficientDet-D0_layer_call_and_return_conditional_losses_84399) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:49.449342 140059710097280 function_deserialization.py:572] Importing a function (__inference_EfficientDet-D0_layer_call_and_return_conditional_losses_88007) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:51.429617 140059710097280 function_deserialization.py:572] Importing a function (__inference_EfficientDet-D0_layer_call_and_return_conditional_losses_80656) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:52.318997 140059710097280 function_deserialization.py:572] Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_57258) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:54.171347 140059710097280 function_deserialization.py:572] Importing a function (__inference_inference_fn_75140) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:55.318600 140059710097280 function_deserialization.py:572] Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_115338) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:57.111330 140059710097280 function_deserialization.py:572] Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_52693) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:11:57.959706 140059710097280 function_deserialization.py:572] Importing a function (__inference_inference_fn_16356) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "W0817 03:12:01.869163 140059710097280 function_deserialization.py:572] Importing a function (__inference_bifpn_layer_call_and_return_conditional_losses_116958) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "2021-08-17 03:12:10.719374: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
            "2021-08-17 03:12:10.719420: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
            "2021-08-17 03:12:10.719430: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored change_concat_input_ranges.\n",
            "2021-08-17 03:12:10.720213: I tensorflow/cc/saved_model/reader.cc:38] Reading SavedModel from: /content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model\n",
            "2021-08-17 03:12:10.893360: I tensorflow/cc/saved_model/reader.cc:90] Reading meta graph with tags { serve }\n",
            "2021-08-17 03:12:10.893430: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model\n",
            "2021-08-17 03:12:11.519126: I tensorflow/cc/saved_model/loader.cc:211] Restoring SavedModel bundle.\n",
            "2021-08-17 03:12:12.579073: I tensorflow/cc/saved_model/loader.cc:195] Running initialization op on SavedModel bundle at path: /content/output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model\n",
            "2021-08-17 03:12:13.098721: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 2378504 microseconds.\n",
            "2021-08-17 03:12:14.605252: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2021-08-17 03:12:28.167264: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1899] Estimated count of arithmetic ops: 6.803 G  ops, equivalently 3.402 G  MACs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5OvnigvxIiE"
      },
      "source": [
        "# **Build edgetpu model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAGPfDqxyaGC",
        "outputId": "e6626733-81a1-4827-e724-c4c634bf56ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "! sudo apt-get update\n",
        "! sudo apt-get install edgetpu-compiler"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  33381      0 --:--:-- --:--:-- --:--:-- 32948\r100  2537  100  2537    0     0  33381      0 --:--:-- --:--:-- --:--:-- 32948\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6,722 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [2,327 B]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Hit:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,790 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [570 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,730 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [916 kB]\n",
            "Fetched 8,482 kB in 4s (2,290 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  edgetpu-compiler\n",
            "0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 7,913 kB of archives.\n",
            "After this operation, 31.2 MB of additional disk space will be used.\n",
            "Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 16.0 [7,913 kB]\n",
            "Fetched 7,913 kB in 0s (47.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package edgetpu-compiler.\n",
            "(Reading database ... 148486 files and directories currently installed.)\n",
            "Preparing to unpack .../edgetpu-compiler_16.0_amd64.deb ...\n",
            "Unpacking edgetpu-compiler (16.0) ...\n",
            "Setting up edgetpu-compiler (16.0) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5myr6QyeMF",
        "outputId": "bff5722b-4704-42c4-ce1e-9fd2fce021a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd $OUTPUT_DIR\n",
        "\n",
        "! edgetpu_compiler -s detect.tflite -m 12"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/output_efficientdet_d0_coco17_tpu-32_hard_hat\n",
            "Edge TPU Compiler version 16.0.384591198\n",
            "Started a compilation timeout timer of 180 seconds.\n",
            "\n",
            "Model compiled successfully in 5051 ms.\n",
            "\n",
            "Input model: detect.tflite\n",
            "Input size: 22.67MiB\n",
            "Output model: detect_edgetpu.tflite\n",
            "Output size: 22.24MiB\n",
            "On-chip memory used for caching model parameters: 0.00B\n",
            "On-chip memory remaining for caching model parameters: 0.00B\n",
            "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
            "Number of Edge TPU subgraphs: 0\n",
            "Total number of operations: 41435\n",
            "Operation log: detect_edgetpu.log\n",
            "\n",
            "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
            "Number of operations that will run on Edge TPU: 0\n",
            "Number of operations that will run on CPU: 41435\n",
            "\n",
            "Operator                       Count      Status\n",
            "\n",
            "SLICE                          20400      Operation is working on an unsupported data type\n",
            "DEPTHWISE_CONV_2D              80         Operation is working on an unsupported data type\n",
            "MEAN                           16         Operation is working on an unsupported data type\n",
            "ADD                            9          Operation is working on an unsupported data type\n",
            "MAX_POOL_2D                    14         Operation is working on an unsupported data type\n",
            "FULLY_CONNECTED                20400      Operation is working on an unsupported data type\n",
            "CONV_2D                        134        Operation is working on an unsupported data type\n",
            "CONCATENATION                  2          Operation is working on an unsupported data type\n",
            "PACK                           48         Tensor has unsupported rank (up to 3 innermost dimensions mapped)\n",
            "PACK                           24         Operation is working on an unsupported data type\n",
            "RESHAPE                        34         Operation is working on an unsupported data type\n",
            "RESHAPE                        36         Tensor has unsupported rank (up to 3 innermost dimensions mapped)\n",
            "CUSTOM                         1          Operation is working on an unsupported data type\n",
            "LOGISTIC                       119        Operation is working on an unsupported data type\n",
            "MUL                            118        Operation is working on an unsupported data type\n",
            "Compilation child process completed within timeout period.\n",
            "Compilation succeeded! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5q-PuAaGRcs",
        "outputId": "31011e0b-4907-4ca2-ec90-9ca106adb02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! cp -r /content/train/ckpt* $OUTPUT_DIR\n",
        "! cp -r /content/inference_graph/* $OUTPUT_DIR\n",
        "\n",
        "%cd /content/\n",
        "! tar cvf output_efficientdet_d0_coco17_tpu-32_hard_hat.tar.gz output_efficientdet_d0_coco17_tpu-32_hard_hat"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-4.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-3.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/detect_edgetpu.tflite\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/detect.tflite\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-2.data-00000-of-00001\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/detect_edgetpu.log\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-1.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-4.data-00000-of-00001\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/variables/\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/variables/variables.data-00000-of-00001\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/variables/variables.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/assets/\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/saved_model/saved_model.pb\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-2.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/pipeline.config\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-3.data-00000-of-00001\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/checkpoint/\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/checkpoint/ckpt-0.data-00000-of-00001\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/checkpoint/ckpt-0.index\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/checkpoint/checkpoint\n",
            "output_efficientdet_d0_coco17_tpu-32_hard_hat/ckpt-1.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}